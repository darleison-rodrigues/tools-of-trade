{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97cba5d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank as a placeholder for future code or markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - Data Formatting for Fine-tuning\n",
    "\n",
    "This notebook focuses on taking your raw extracted text and external datasets, and transforming them into the specific prompt-response format required for fine-tuning your LLM.\n",
    "\n",
    "## Objective:\n",
    "- Load extracted documents from `data/processed/extracted_raw_documents.jsonl`.\n",
    "- Load any additional external datasets from `data/external/`.\n",
    "- Apply a strategy (e.g., manual annotation, heuristic-based, or self-instruct) to create high-quality prompt-response pairs.\n",
    "- Format these pairs into the `{\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"model\", \"content\": \"...\"}]}` structure.\n",
    "- Save the final fine-tuning dataset to `data/processed/fine_tuning_data.jsonl`.\n",
    "\n",
    "## Instructions:\n",
    "1.  **Load your extracted data:** Use the `json` module to load `extracted_raw_documents.jsonl`.\n",
    "2.  **Load external datasets:** Use the `datasets` library to load any datasets from `data/external/`.\n",
    "3.  **Implement your formatting logic:** This is the most custom part. You'll write Python code to generate your prompt-response pairs.\n",
    "4.  **Save the dataset:** Ensure the output is a JSON Lines file.\n",
    "\n",
    "## Code (Example of what you'd put here):\n",
    "\n",
    "```python\n",
    "# import sys\n",
    "# sys.path.append('scripts')\n",
    "# from ingest_data import PROCESSED_DOCS_PATH\n",
    "# import json\n",
    "# from datasets import Dataset\n",
    "# import os\n",
    "\n",
    "# # Define output path\n",
    "# FINE_TUNING_DATA_PATH = os.path.join(os.path.dirname(PROCESSED_DOCS_PATH), \"fine_tuning_data.jsonl\")\n",
    "\n",
    "# # Load extracted raw documents\n",
    "# extracted_docs = []\n",
    "# try:\n",
    "#     with open(PROCESSED_DOCS_PATH, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             extracted_docs.append(json.loads(line))\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: {PROCESSED_DOCS_PATH} not found. Please run ingest_data.py first.\")\n",
    "\n",
    "# # --- Example: Simple formatting for a Q&A chatbot from extracted text ---\n",
    "# # This is a highly simplified example. You'd need more sophisticated logic\n",
    "# # to create meaningful Q&A pairs from arbitrary text.\n",
    "# # For code, you might prompt: \"Explain this function: [code snippet]\"\n",
    "# # For articles: \"Summarize this paragraph: [paragraph]\"\n",
    "\n",
    "# formatted_data = []\n",
    "# for doc in extracted_docs:\n",
    "#     content = doc.get(\"content\", \"\")\n",
    "#     source = doc.get(\"source_filename\", \"unknown\")\n",
    "#     file_type = doc.get(\"file_type\", \"text\")\n",
    "\n",
    "#     # Simple heuristic: take first 200 chars as context, ask a generic question\n",
    "#     if len(content) > 100:\n",
    "#         context_snippet = content[:200] + \"...\"\n",
    "#         # Example for text/pdf\n",
    "#         if file_type in [\"text\", \"pdf\"]:\n",
    "#             user_prompt = f\"Based on this snippet from '{source}': '{context_snippet}', what is the main idea?\"\n",
    "#             model_response = f\"The main idea of this snippet from '{source}' is about [LLM-generated main idea].\"\n",
    "#             # In a real scenario, you'd use an LLM (self-instruct) or manual annotation\n",
    "#             # to generate the 'model_response' based on the 'context_snippet'.\n",
    "#             formatted_data.append({\n",
    "#                 \"messages\": [\n",
    "#                     {\"role\": \"user\", \"content\": user_prompt},\n",
    "#                     {\"role\": \"model\", \"content\": model_response}\n",
    "#                 ]\n",
    "#             })\n",
    "#         # Example for code\n",
    "#         elif file_type == \"code\":\n",
    "#             user_prompt = f\"Explain the purpose of the code snippet from '{source}':\\n```python\\n{content[:200]}...\\n```\"\n",
    "#             model_response = f\"This code snippet from '{source}' appears to [LLM-generated explanation].\"\n",
    "#             formatted_data.append({\n",
    "#                 \"messages\": [\n",
    "#                     {\"role\": \"user\", \"content\": user_prompt},\n",
    "#                     {\"role\": \"model\", \"content\": model_response}\n",
    "#                 ]\n",
    "#             })\n",
    "\n",
    "# # Save the formatted data\n",
    "# os.makedirs(os.path.dirname(FINE_TUNING_DATA_PATH), exist_ok=True)\n",
    "# with open(FINE_TUNING_DATA_PATH, 'w', encoding='utf-8') as f:\n",
    "#     for item in formatted_data:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# print(f\"Created {len(formatted_data)} fine-tuning examples and saved to {FINE_TUNING_DATA_PATH}\")\n",
    "\n",
    "# # Optional: Load into Hugging Face Dataset format for inspection\n",
    "# # hf_dataset = Dataset.from_list(formatted_data)\n",
    "# # print(\"\\nSample of Hugging Face Dataset:\")\n",
    "# # print(hf_dataset[0])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
